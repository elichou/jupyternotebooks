{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "#%matplotlib notebook\n",
    "from IPython import display\n",
    "import pandas as pd\n",
    "\n",
    "import sys, subprocess\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from armcodlib import *\n",
    "import keras.backend as K\n",
    "import os\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import animation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create arm positions dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the training data: \n",
    "- train_posture_before : joint angles before command\n",
    "- train_posture_after  : joint angles after command is applied\n",
    "- command : command applied\n",
    "- train_position_before : end effector position before command\n",
    "- train_position_after : end effector position after command is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_POSTURE = 50\n",
    "NB_COMMAND = 100\n",
    "NB_DATA = NB_POSTURE*NB_COMMAND\n",
    "BATCH_SIZE = 100\n",
    "TEST_BUF = 1000\n",
    "IMG_SIZE = 64\n",
    "DIMS = (IMG_SIZE, IMG_SIZE,2)\n",
    "N_TRAIN_BATCHES =int(NB_DATA/BATCH_SIZE)\n",
    "N_TEST_BATCHES = int(TEST_BUF/BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "train_posture_before, train_posture_after, train_command, train_position_before, train_position_after  = create_random_data(NB_POSTURE, NB_COMMAND, \"train\")\n",
    "train_images = load_and_process_images(NB_DATA, \"train\")\n",
    "#test_posture_before, test_posture_after, test_command, test_position_before, test_position_after = create_random_data(10,10, \"test\")\n",
    "#test_images = load_and_process_images(100, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the training data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_images_pickle = open(\"pickles/train_images.pickle\", \"wb\")\n",
    "train_command_pickle = open(\"pickles/train_command.pickle\", \"wb\")\n",
    "train_posture_before_pickle = open(\"pickles/train_posture_before.pickle\", \"wb\")\n",
    "train_posture_after_pickle = open(\"pickles/train_posture_after.pickle\", \"wb\")\n",
    "train_position_before_pickle = open(\"pickles/train_position_before.pickle\", \"wb\")\n",
    "train_position_after_pickle = open(\"pickles/train_position_after.pickle\", \"wb\")\n",
    "\n",
    "pickle.dump(train_images, train_images_pickle)\n",
    "pickle.dump(train_command, train_command_pickle)\n",
    "pickle.dump(train_posture_before, train_posture_before_pickle)\n",
    "pickle.dump(train_posture_after, train_posture_after_pickle)\n",
    "pickle.dump(train_position_before, train_position_before_pickle)\n",
    "pickle.dump(train_position_after, train_position_after_pickle)\n",
    "\n",
    "train_images_pickle.close()\n",
    "train_command_pickle.close()\n",
    "train_posture_before_pickle.close()\n",
    "train_posture_after_pickle.close()\n",
    "train_position_after_pickle.close()\n",
    "train_position_before_pickle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_pickle = open(\"pickles/train_images.pickle\", \"rb\")\n",
    "train_command_pickle = open(\"pickles/train_command.pickle\", \"rb\")\n",
    "train_posture_before_pickle = open(\"pickles/train_posture_before.pickle\", \"rb\")\n",
    "train_posture_after_pickle = open(\"pickles/train_posture_after.pickle\", \"rb\")\n",
    "train_position_before_pickle = open(\"pickles/train_position_before.pickle\",\"rb\")\n",
    "train_position_after_pickle = open(\"pickles/train_position_after.pickle\", \"rb\")\n",
    "\n",
    "\n",
    "\n",
    "train_command = pickle.load(train_command_pickle)\n",
    "train_images = pickle.load(train_images_pickle)\n",
    "train_posture_before = pickle.load(train_posture_before_pickle)\n",
    "train_posture_after = pickle.load(train_posture_after_pickle)\n",
    "train_position_before = pickle.load(train_position_before_pickle)\n",
    "train_position_after = pickle.load(train_position_after_pickle)\n",
    "\n",
    "train_images_pickle.close()\n",
    "train_command_pickle.close()\n",
    "train_posture_after_pickle.close()\n",
    "train_posture_before_pickle.close()\n",
    "train_position_before_pickle.close()\n",
    "train_position_after_pickle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the tf.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_images[:,:,:,],train_images[:,:,:,1:]))\n",
    "    .repeat(10)\n",
    "    .shuffle(NB_DATA)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "train_dataset_x = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_images[:,:,:,],train_images[:,:,:,:0]))\n",
    "    .repeat(10)\n",
    "    .shuffle(NB_DATA)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "visu_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_images[:,:,:,]))\n",
    "    .repeat(10)\n",
    "    .shuffle(NB_DATA)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder = build_dense_encoder()\n",
    "decoder = build_dense_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "autoencoder = tf.keras.Model(encoder.input, decoder(encoder(encoder.input)), name = \"autoencoder\")\n",
    "\n",
    "autoencoder.compile(optimizer = optimizer, \n",
    "             loss='mse',\n",
    "             metrics=['accuracy','kullback_leibler_divergence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = autoencoder.fit(train_dataset, \n",
    "                    epochs = 50,\n",
    "                    steps_per_epoch = 10,\n",
    "                    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example_data = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "imshow(autoencoder.predict(example_data)[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imshow(example_data[0][0,:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main convolutional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_conv = build_conv2D_pointwise_encoder()\n",
    "decoder_dense = build_dense_pointwise_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "autoencoder_conv = tf.keras.Model(encoder_conv.input, decoder_dense(encoder_conv(encoder_conv.input)), name = \"autoencoder\")\n",
    "\n",
    "autoencoder_conv.compile(optimizer = 'adam', \n",
    "             loss='mse',\n",
    "             metrics=['accuracy','kullback_leibler_divergence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = autoencoder_conv.fit(train_dataset, \n",
    "                    epochs = 10)#, \n",
    "                    #steps_per_epoch = 50)\n",
    "                    #callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = encoder_conv.predict(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(autoencoder_conv.predict(example_data)[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imshow(example_data[0][0,:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of ConvNet Filters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = encoder_conv  \n",
    "layer_name = 'conv_y_2'\n",
    "input_layer = 'encoder_input'\n",
    "intermediate_layer_model = tf.keras.Model(inputs=model.get_layer(input_layer).input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(example_data)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape = (64,64,1)))\n",
    "model.add(intermediate_layer_model.layers[3])\n",
    "model.add(intermediate_layer_model.layers[4])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(31):\n",
    "    plt.figure()\n",
    "    plt.imshow(intermediate_output[0][:,:,i])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "results = plot_and_compute_conv_filters(model)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(results[:,:,0])\n",
    "savefig('images/filter_conv_response.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of dense layers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = encoder_conv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "size = 64\n",
    "margin = 5\n",
    "tt , results = plot_and_compute_last_filters(model, nb_pass=100000)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(results[:,:,0])\n",
    "savefig('images/filter_response.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t , color_position = compute_latent_filters(model, train_command, visu_dataset.__iter__(), 5000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tsne = TSNE(n_components=2, random_state=0, perplexity=50, early_exaggeration=4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "g = []\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    tmp = tf.keras.backend.flatten(t[i])\n",
    "    g.append(tmp[32:])\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "res = tsne.fit_transform(g)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "nb_classes = 8\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    label_id = color_position[i] \n",
    "    plt.scatter(res[i,0], \n",
    "               res[i,1], \n",
    "               color= plt.cm.Set1(label_id / float(nb_classes)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_cmd = []\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    tmp0 = tf.keras.backend.flatten(train_command[i])\n",
    "    tmp1 = tf.keras.backend.flatten(train_position_before[i])\n",
    "    tmp2 = tf.keras.backend.flatten(train_position_after[i])\n",
    "    tmp3 = tf.keras.backend.concatenate([tmp0, tmp1, tmp2])\n",
    "    train_cmd.append(tmp3)\n",
    "tsnee = TSNE(n_components=2, random_state=0, perplexity=50, early_exaggeration=4)\n",
    "tmp = tsnee.fit_transform(train_cmd)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "nb_classes = 8\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    label_id = color_position[i] \n",
    "    plt.scatter(tmp[i,0], \n",
    "               tmp[i,1], \n",
    "               color= plt.cm.Set1(label_id / float(nb_classes)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "plot_and_save_visual_direction(train_position_before, train_position_after)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "images = []\n",
    "for image_path in glob(os.path.join('images/visual_direction', \"*.png\")):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (150, 150))\n",
    "    #image = segment_plant(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(image, (45,45))\n",
    "    \n",
    "    image = image.flatten()\n",
    "        \n",
    "    images.append(image)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "images = np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D t-SNE animation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "images_scaled = StandardScaler().fit_transform(g)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pca = PCA(n_components=32)\n",
    "pca_result = pca.fit_transform(images_scaled)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tsne = TSNE(n_components=2, perplexity=40.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tsne_result = tsne.fit_transform(pca_result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tsne_result_scaled = StandardScaler().fit_transform(tsne_result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tsne = TSNE(n_components=3)\n",
    "tsne_result = tsne.fit_transform(pca_result)\n",
    "tsne_result_scaled = StandardScaler().fit_transform(tsne_result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "%matplotlib notebook\n",
    "nb_classes = 2\n",
    "fig = plt.figure(figsize=(25,25))\n",
    "ax = fig.add_subplot(111,projection='3d')\n",
    "\n",
    "plt.grid()\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    label_id = color_position[i] #argmax(g[i])\n",
    "    ax.scatter(tsne_result_scaled[i,0], \n",
    "               tsne_result_scaled[i,1], \n",
    "               tsne_result_scaled[i,2], \n",
    "               color= plt.cm.Set1(label_id / float(nb_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def image_to_convnet(custom_shape = (IMG_SIZE, IMG_SIZE,1)):\n",
    "    inputs = tf.keras.Input(shape = custom_shape, name = 'conv_input')\n",
    "    x = tf.keras.layers.Conv2D(filters = 32, \n",
    "                              kernel_size = 3,\n",
    "                              strides = (2,2),\n",
    "                              activation = 'relu',\n",
    "                              name = 'conv_1')(inputs)\n",
    "    x = tf.keras.layers.Conv2D(filters = 64,\n",
    "                              kernel_size = 3,\n",
    "                              strides = (2,2),\n",
    "                              activation = 'relu',\n",
    "                              name = 'conv_2')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(32, name = 'dense_layer_1')(x)\n",
    "    outputs = tf.keras.layers.Reshape((32,1))(x)\n",
    "    \n",
    "    convnet = tf.keras.Model(inputs = inputs, \n",
    "                            outputs = outputs,\n",
    "                            name = 'conv_net_1')\n",
    "    return convnet\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def pos_to_dense(custom_shape = (1,3)):\n",
    "    inputs = tf.keras.layers.Input(shape = custom_shape, name = 'dense_input')\n",
    "    x = tf.keras.layers.Reshape(custom_shape)(inputs)\n",
    "    x = tf.keras.layers.Dense(32, name = 'dense_1')(x)\n",
    "    x = tf.keras.layers.Dense(32, name = 'dense_2')(x)\n",
    "    outputs = tf.keras.layers.Reshape((32,1))(x)\n",
    "    \n",
    "    densenet = tf.keras.Model(inputs = inputs, outputs = outputs, name = \"dense_net\")\n",
    "    \n",
    "    return densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_model():\n",
    "    \"\"\" visual_direction and posture before as input\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(2,3))\n",
    "    \n",
    "    h = tf.keras.layers.Lambda(lambda x: x[:,0,:])(inputs)\n",
    "    p = tf.keras.layers.Lambda(lambda x: x[:,1,:3])(inputs)\n",
    "    \n",
    "    h = tf.keras.layers.Reshape((1,3))(h)\n",
    "    p = tf.keras.layers.Reshape((1,3))(p)\n",
    "\n",
    "    fh = tf.keras.layers.Flatten()(h)\n",
    "    fh = tf.keras.layers.Dense(32, name = 'dense_h_1')(fh)\n",
    "    fh = tf.keras.layers.Dense(32, name = 'dense_h_2')(fh)\n",
    "    fh = tf.keras.layers.Reshape((32, 1))(fh)\n",
    "    \n",
    "    fp = tf.keras.layers.Flatten()(p)\n",
    "    fp = tf.keras.layers.Dense(32, name = 'dense_p_1')(fp)\n",
    "    fp = tf.keras.layers.Dense(32, name = 'dense_p_2')(fp)\n",
    "    fp = tf.keras.layers.Reshape((32,1))(fp)\n",
    "    \n",
    "    matmul = tf.keras.layers.Multiply()([fp, fh])\n",
    "    \n",
    "    fy = tf.keras.layers.Flatten()(matmul)\n",
    "    fy = tf.keras.layers.Dense(LATENT_DIM, name = 'latent_y_1')(fy)\n",
    "    fy = tf.keras.layers.Dense(LATENT_DIM, name = 'latent_y_2')(fy)\n",
    "    fy = tf.keras.layers.Dense(3, name = 'latent_y_out')(fy)\n",
    "    fy = tf.keras.layers.Reshape((1,3))(fy)\n",
    "    \n",
    "    outputs = fy \n",
    "    \n",
    "    model = tf.keras.Model(inputs = inputs, outputs = outputs, name='control_model')\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = control_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "model.compile(optimizer = 'adam', \n",
    "             loss='mse',\n",
    "             metrics=['accuracy','kullback_leibler_divergence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(joint_command, train_posture_before, train_posture_after, train_position_after, train_position_before):\n",
    "    \n",
    "    t_before = map(lambda x : x[0,:3], train_posture_before)\n",
    "    \n",
    "    t_command = map(lambda x : x[0,:3], train_command)\n",
    "   \n",
    "    t_visual_direction = train_position_after - train_position_before   \n",
    "\n",
    "    t_before = np.expand_dims(t_before, 1)\n",
    "\n",
    "    t_command = np.expand_dims(t_command, 1)\n",
    " \n",
    "    tmp_input = np.concatenate([t_visual_direction, t_before], axis = 1)\n",
    "    \n",
    "    train_control_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((tmp_input, t_command))\n",
    "        .repeat(10)\n",
    "        .shuffle(NB_DATA)\n",
    "        .batch(BATCH_SIZE)\n",
    "        )\n",
    "    \n",
    "    return train_control_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_control_dataset = prepare_dataset(train_command, \n",
    "                                        train_posture_before,\n",
    "                                       train_posture_after, \n",
    "                                       train_position_after, \n",
    "                                       train_position_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "history = model.fit(train_control_dataset, \n",
    "                    epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_visuomotor_control(control_model, visual_direction):\n",
    "    # liste des postures successives\n",
    "    postures = []\n",
    "    # choisir une posture initiale\n",
    "    #posture = (np.random.random((1,3)))\n",
    "    posture = np.array([[0.3,0.3,0.2]])\n",
    "    postures.append(posture)\n",
    "\n",
    "    for i in range(200):\n",
    "        tmp = posture\n",
    "\n",
    "        inputs = np.concatenate([visual_direction, tmp], axis = 0)\n",
    "      \n",
    "        inputs = np.expand_dims(inputs, 0)\n",
    "\n",
    "        command = control_model.predict(inputs)\n",
    "\n",
    "        posture = posture + command[0]/10   \n",
    "\n",
    "        posture = valid_posture(posture)\n",
    "        postures.append(posture)\n",
    "\n",
    "    return postures\n",
    " \n",
    "def valid_posture(posture):\n",
    "    valid_posture = posture\n",
    "    \n",
    "    for i in range(len(posture)):\n",
    "        if posture[0][i] > 2*pi/3:\n",
    "            valid_posture[0][i] = 2*pi/3\n",
    "        if posture[0][i] < -2*pi/3:\n",
    "            valid_posture[0][i] = -2*pi/3\n",
    "    return valid_posture\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postures = test_visuomotor_control(model, [[0,0,0]] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_arm_from_posture(posture):\n",
    "    \"\"\"function ploting and saving in /images 3d arm plot\n",
    "    from arg joint state\n",
    "\n",
    "    Args :\n",
    "        phi1, phi2, theta1 : joint angles (3 dof arm )\n",
    "\n",
    "    Returns :\n",
    "        ax : a matplotlib figure object\n",
    "    \"\"\"\n",
    "    phi1 = posture[0][0]\n",
    "    phi2 = posture[0][1]\n",
    "    theta1 = posture[0][2]\n",
    "    \n",
    "    fig = figure(facecolor=(0.0, 0.0, 0.0))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    x = [0, 0, L1 * cos(phi1) * cos(theta1), L2 *\n",
    "         cos(phi1 + phi2) * cos(theta1)]\n",
    "    y = [0, 0, L1 * cos(phi1) * sin(theta1), L2 *\n",
    "         cos(phi1 + phi2) * sin(theta1)]  # ELBOW + HAND\n",
    "    z = [0, 0, L1 * sin(phi1), L2 * sin(phi1 + phi2)]  # ELBOW + HAND\n",
    "    # ax.plot(x[0:1], y[0:1], z[0:1], label='shoulder', lw=2, color= 'k')\n",
    "    # ax.plot(x[2:3], y[2:3], z[2:3], label='elbow', lw=2, color= 'c')\n",
    "    # Hide grid lines\n",
    "    ax.grid(False)\n",
    "    # ax.set_autoscale_on(True)\n",
    "    ax.set_facecolor((0.0, 0.0, 0.0))\n",
    "\n",
    "    ax.set_xlim(left=-0.2, right=0.2)\n",
    "    ax.set_ylim(bottom=-0.2, top=0.2)\n",
    "    ax.set_zlim(bottom=-0.2, top=0.2)\n",
    "    ax.axis('off')\n",
    "    # Hide axes ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_zticks([])\n",
    "    ax.plot(x, y, z, label='shoulder', lw=5, color='white')\n",
    "    #filename = 'images/%s.png' % time\n",
    "    #savefig(filename, facecolor=fig.get_facecolor(), edgecolor='none')\n",
    "    \n",
    "def plot_elbow_from_posture(posture):\n",
    "    \"\"\"function ploting and saving in /images 3d arm plot\n",
    "    from arg joint state\n",
    "\n",
    "    Args :\n",
    "        phi1, phi2, theta1 : joint angles (3 dof arm )\n",
    "\n",
    "    Returns :\n",
    "        ax : a matplotlib figure object\n",
    "    \"\"\"\n",
    "    phi1 = posture[0][0]\n",
    "    phi2 = posture[0][1]\n",
    "    theta1 = posture[0][2]\n",
    "    \n",
    "    fig = figure(facecolor=(0.0, 0.0, 0.0))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    x = [0, 0, L1 * cos(phi1) * cos(theta1)]\n",
    "    y = [0, 0, L1 * cos(phi1) * sin(theta1)]  # ELBOW + HAND\n",
    "    z = [0, 0, L1 * sin(phi1)]  # ELBOW + HAND\n",
    "    # ax.plot(x[0:1], y[0:1], z[0:1], label='shoulder', lw=2, color= 'k')\n",
    "    # ax.plot(x[2:3], y[2:3], z[2:3], label='elbow', lw=2, color= 'c')\n",
    "    # Hide grid lines\n",
    "    ax.grid(False)\n",
    "    # ax.set_autoscale_on(True)\n",
    "    ax.set_facecolor((0.0, 0.0, 0.0))\n",
    "\n",
    "    ax.set_xlim(left=-0.2, right=0.2)\n",
    "    ax.set_ylim(bottom=-0.2, top=0.2)\n",
    "    ax.set_zlim(bottom=-0.2, top=0.2)\n",
    "    ax.axis('off')\n",
    "    # Hide axes ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_zticks([])\n",
    "    ax.plot(x, y, z, label='shoulder', lw=5, color='white')\n",
    "    #filename = 'images/%s.png' % time\n",
    "    #savefig(filename, facecolor=fig.get_facecolor(), edgecolor='none')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(len(postures)):\n",
    "    plot_elbow_from_posture(postures[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_robot_1(angles):\n",
    "    \"\"\"function calculating end effector postion from joint angles\n",
    "\n",
    "    Args :\n",
    "        phi1, phi2, theta1, theta2 : joint angles (4 dof arm )\n",
    "\n",
    "    Returns:\n",
    "        x,y,z : end effector position\n",
    "        vx,vy,vz : speed i guess...\n",
    "    \"\"\"\n",
    "    phi1, phi2, theta1 = angles\n",
    "    theta2 = 0\n",
    "    x = L1 * cos(phi1) * cos(theta1) + L2 * cos(phi1 + phi2) * \\\n",
    "        cos(theta1 + theta2)  # ELBOW + HAND\n",
    "    y = L1 * cos(phi1) * sin(theta1) + L2 * cos(phi1 + phi2) * \\\n",
    "        sin(theta1 + theta2)  # ELBOW + HAND\n",
    "    z = L1 * sin(phi1) + L2 * sin(phi1 + phi2)  # ELBOW + HAND\n",
    "    vx = L2 * cos(phi1 + phi2) * cos(theta1 + theta2)\n",
    "    vy = L2 * cos(phi1 + phi2) * sin(theta1 + theta2)\n",
    "    vz = L2 * sin(phi1 + phi2)\n",
    "    return [x, y, z]\n",
    "\n",
    "def control_robot_elbow(angles):\n",
    "    phi1, phi2, theta1 = angles\n",
    "    x = L1 * cos(phi1) * cos(theta1)\n",
    "    y = L1 * cos(phi1) * sin(theta1)\n",
    "    z = L1 * sin(phi1)\n",
    "    return np.array([x,y,z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trajectory(postures):\n",
    "    tmp = []\n",
    "    for i in range(len(postures)):\n",
    "        tmp.append(control_robot_1(postures[i][0]))\n",
    "    return tmp\n",
    "t = compute_trajectory(postures)\n",
    "tprime = np.array(t)\n",
    "#print tprime[:,2]\n",
    "\n",
    "def compute_elbow_trajectory(postures):\n",
    "    tmp = []\n",
    "    for i in range(len(postures)):\n",
    "        tmp.append(control_robot_elbow(postures[i][0]))\n",
    "    return np.array(tmp)\n",
    "t = compute_elbow_trajectory(postures)\n",
    "print t[:,2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.set_xlim(left=-1, right=1)\n",
    "ax.set_ylim(bottom=-1, top=1)\n",
    "ax.set_zlim(bottom=-1, top=1)\n",
    "ax.plot(t[:200,0], t[:200,1], t[:200,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.set_xlim(left=-1, right=1)\n",
    "ax.set_ylim(bottom=-1, top=1)\n",
    "ax.set_zlim(bottom=-1, top=1)\n",
    "ax.plot(tprime[:200,0], tprime[:200,1], tprime[:200,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_to_position(control_model, target_position):\n",
    "    postures = []\n",
    "    current_posture = valid_posture(np.random.random((1,3)))\n",
    "\n",
    "    visual_direction = compute_vd_from_position(target_position, current_posture)\n",
    "    postures.append(current_posture)\n",
    "    j = 0\n",
    "    while (distance_end_effector_to_target(visual_direction)) and (j < 100):\n",
    "        \n",
    "        inputs = np.expand_dims(np.concatenate([visual_direction, current_posture], axis=0), 0)\n",
    "        \n",
    "        new_command = control_model.predict(inputs)\n",
    "       \n",
    "        current_posture = current_posture + new_command[0]/10\n",
    "        current_posture = valid_posture(current_posture)\n",
    "        \n",
    "        visual_direction = compute_vd_from_position(target_position, current_posture)\n",
    "        postures.append(current_posture)\n",
    "        j += 1\n",
    "        \n",
    "def compute_vd_from_position(target_position, current_posture):\n",
    "    print 'current posture ', shape(current_posture)\n",
    "    current_position = control_robot_1(current_posture[0])\n",
    "    current_position  = np.expand_dims(current_position, 0)\n",
    "    return np.subtract(target_position, current_position)\n",
    "\n",
    "def distance_end_effector_to_target(visual_direction):\n",
    "    dx, dy, dz = visual_direction\n",
    "    \n",
    "    dist = np.sqrt(dx*dx+dy*dy+dz*dz)\n",
    "    \n",
    "    return (dist > 0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_to_position(model, [[0.,0.,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_latent_control_filters(model, iterator):\n",
    "    t = []\n",
    "    for i in tqdm.tqdm(range(5000)):\n",
    "        tmp = iterator.get_next()\n",
    "        tmp = tf.expand_dims(tmp, 0)\n",
    "        \n",
    "        t.append(model.predict(tmp))\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu_control_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((tmp3))\n",
    "    .repeat(10)\n",
    "    .shuffle(NB_DATA)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compute_latent_control_filters(model, visu_control_dataset.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpp = []\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    tmpp.append(res[i][0][0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0, perplexity=50, early_exaggeration=4)\n",
    "tsne_res = tsne.fit_transform(tmpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "nb_classes = 8\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    label_id = color_position[i] \n",
    "    plt.scatter(tsne_res[i,0], \n",
    "               tsne_res[i,1],\n",
    "            color= plt.cm.Set1(label_id / float(nb_classes)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d visualization of neural activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "def select_random_posture():\n",
    "    posture = zeros(4)\n",
    "    posture[0] = randrange(1,0,2*pi/3)\n",
    "    posture[1] = randrange(1,0,2*pi/3)\n",
    "    posture[2] = randrange(1,0,2*pi/3)\n",
    "    return posture\n",
    "\n",
    "def select_lin_posture(n):\n",
    "    position = zeros((10*n*n, 4))\n",
    "    phi1 = linspace(0,2*pi/3,n)\n",
    "    phi2 = linspace(0,2*pi/3,n)\n",
    "    theta1 = linspace(0,2*pi/3,n)\n",
    "    xx, yy, zz = meshgrid(phi1, phi2, theta1)\n",
    "    position[:,0] = xx.flatten()\n",
    "    position[:,1] = yy.flatten()\n",
    "    position[:,2] = zz.flatten()\n",
    "    \n",
    "    return position \n",
    "\n",
    "def select_lin_command(n):\n",
    "    commands = zeros((10*n*n, 4))\n",
    "    c0 = linspace(-1,1,n)*0.2\n",
    "    c1 = linspace(-1,1,n)*0.2\n",
    "    c2 = linspace(-1,1,n)*0.2\n",
    "    cc0, cc1, cc2 = meshgrid(c0,c1,c2)\n",
    "    commands[:,0] = cc0.flatten()\n",
    "    commands[:,1] = cc1.flatten()\n",
    "    commands[:,2] = cc2.flatten()\n",
    "    \n",
    "    return commands\n",
    "    \n",
    "def select_command():\n",
    "    command = zeros(4)\n",
    "    command[0] = randrange(1,-1, 1) * 0.25\n",
    "    command[1] = randrange(1,-1, 1) * 0.25\n",
    "    command[2] = random.choice(\n",
    "        [-1, 1]) * np.sqrt(0.375 - (command[0] * command[0] + command[1] * command[1]))\n",
    "    return command\n",
    "\n",
    "def create_image_visu(posture, i):\n",
    "    phi1, phi2, theta1 = posture[0], posture[1], posture[2]\n",
    "    name = \"visualization/%s\" %i\n",
    "    plot_arm(phi1, phi2, theta1, name)\n",
    "    \n",
    "def compute_random_activity_for_position(model, i):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    \n",
    "    for k in (range(300)):\n",
    "        posture_before = select_random_posture()\n",
    "        position_before = control_robot(posture_before)\n",
    "        name_before = \"fig_before_%s\" %k\n",
    "        create_image_visu(posture_before, name_before)\n",
    "        \n",
    "        tmp0 = \"images/visualization/\" + name_before + \".png\"\n",
    "        \n",
    "        tens_before = load_and_preprocess_image(tmp0)\n",
    "        noised_tens_before = noised_image(tens_before)\n",
    "        \n",
    "        for j in range(10):\n",
    "            res = []\n",
    "            command = select_command()\n",
    "            posture_after = posture_before + command\n",
    "            position_after = control_robot(posture_after)\n",
    "            name_after = \"fig_after_%s_%s\" %(k, j)\n",
    "            create_image_visu(posture_after, name_after)\n",
    "            \n",
    "            tmp1 = \"images/visualization/\" + name_after + \".png\"\n",
    "\n",
    "            tens_after = load_and_preprocess_image(tmp1)\n",
    "            noised_tens_after = noised_image(tens_after)\n",
    "            \n",
    "            t = tf.concat([noised_tens_before, noised_tens_after], -1)\n",
    "            r = tf.reshape(t, [1,IMG_SIZE, IMG_SIZE, 2])\n",
    "            \n",
    "            \n",
    "            output = model.predict(r)\n",
    "            res.append(output[0][0][i])\n",
    "        \n",
    "        moy = mean(res)\n",
    "       \n",
    "        ax.scatter3D(position_before[0], position_before[1], position_before[2], color= plt.cm.Greens((moy) *2))\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "def compute_activity_for_position(model, i):\n",
    "    \"\"\" compute averaged-command initial position-specific neural activity\n",
    "    \n",
    "    Args : \n",
    "        model : a keras model object\n",
    "        i : neuron index\n",
    "    \n",
    "    Returns : \n",
    "        null\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    postures = select_lin_posture(10)\n",
    "    command = select_command()\n",
    "    for k in (range(300)):\n",
    "        \n",
    "        posture_before = postures[k]\n",
    "        position_before = control_robot(posture_before)\n",
    "        name_before = \"fig_before_%s\" %k\n",
    "        create_image_visu(posture_before, name_before)\n",
    "        \n",
    "        tmp0 = \"images/visualization/\" + name_before + \".png\"\n",
    "        \n",
    "        tens_before = load_and_preprocess_image(tmp0)\n",
    "        noised_tens_before = noised_image(tens_before)\n",
    "        \n",
    "        for j in range(10):\n",
    "            res = []\n",
    "            \n",
    "            posture_after = posture_before + command\n",
    "            position_after = control_robot(posture_after)\n",
    "            name_after = \"fig_after_%s_%s\" %(k, j)\n",
    "            create_image_visu(posture_after, name_after)\n",
    "            \n",
    "            tmp1 = \"images/visualization/\" + name_after + \".png\"\n",
    "\n",
    "            tens_after = load_and_preprocess_image(tmp1)\n",
    "            noised_tens_after = noised_image(tens_after)\n",
    "            \n",
    "            t = tf.concat([noised_tens_before, noised_tens_after], -1)\n",
    "            r = tf.reshape(t, [1,IMG_SIZE, IMG_SIZE, 2])\n",
    "            \n",
    "            \n",
    "            output = model.predict(r)\n",
    "            res.append(output[0][0][i])\n",
    "        \n",
    "        moy = mean(res)\n",
    "       \n",
    "        ax.scatter3D(position_before[0], position_before[1], position_before[2], color= plt.cm.seismic((moy)))\n",
    "        \n",
    "    plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute_activity_for_position(encoder_conv, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def compute_loss_vd(model, pos_init, joint_command, filter_index):\n",
    "    img = generate_input_from_vd(pos_init,  joint_command)\n",
    "    output = model.predict(img)\n",
    "    loss = output[0][0][filter_index]\n",
    "    \n",
    "    return loss\n",
    "\n",
    "#@tf.function\n",
    "def generate_img_from_pos(phi1, phi2, theta1):\n",
    "    \n",
    "    fig = figure(facecolor=(0.0, 0.0, 0.0))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    phi1 = tf.cast(phi1, tf.float32)\n",
    "    phi2 = tf.cast(phi2, tf.float32)\n",
    "    theta1 = tf.cast(theta1, tf.float32)\n",
    "\n",
    "    x = [0, 0, L1 * tf.math.cos(phi1) * tf.math.cos(theta1), L2 *\n",
    "         tf.math.cos(phi1 + phi2) * tf.math.cos(theta1)]\n",
    "    y = [0, 0, L1 * tf.math.cos(phi1) * tf.math.sin(theta1), L2 *\n",
    "         tf.math.cos(phi1 + phi2) * tf.math.sin(theta1)]  # ELBOW + HAND\n",
    "    z = [0, 0, L1 * tf.math.sin(phi1), L2 * tf.math.sin(phi1 + phi2)]  # ELBOW + HAND\n",
    "   \n",
    "    ax.grid(False)\n",
    "    ax.set_facecolor((0.0, 0.0, 0.0))\n",
    "    ax.set_xlim(left=-0.2, right=0.2)\n",
    "    ax.set_ylim(bottom=-0.2, top=0.2)\n",
    "    ax.set_zlim(bottom=-0.2, top=0.2)\n",
    "    ax.axis('off')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_zticks([])\n",
    "    ax.plot(x, y, z, label='shoulder', lw=5, color='white')\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    x = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "    x = np.reshape(x, (288,432,4))\n",
    "    \n",
    "    tmp = tf.image.resize(x, [IMG_SIZE, IMG_SIZE])\n",
    "    #tmp = tf.reshape(tmp, [1,64,64,4])\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "\n",
    "def generate_input_from_vd(pos_before, joint_command):\n",
    "\n",
    "    pos_before = tf.reshape(pos_before, [1,4])\n",
    "    img_before = generate_img_from_pos(pos_before[0][0],\n",
    "                                      pos_before[0][1],\n",
    "                                      pos_before[0][2])\n",
    "    \n",
    "    pos_after = np.add(pos_before , joint_command)\n",
    "    img_after = generate_img_from_pos(pos_after[0][0],\n",
    "                                      pos_after[0][1],\n",
    "                                      pos_after[0][2])\n",
    "    \n",
    "    t =  tf.concat([img_before[:,:,:1], img_after[:,:,:1]], -1)\n",
    "    t = tf.reshape(t, [1, IMG_SIZE, IMG_SIZE, 2])\n",
    "    \n",
    "    return t\n",
    "\n",
    "def generate_max_vd(model, filter_index, nb_pass):\n",
    "    \n",
    "    list_pos_before = []\n",
    "    list_loss = []\n",
    "    list_joint_command = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(100)):\n",
    "        \n",
    "        joint_command = tf.convert_to_tensor(\n",
    "            np.random.random((1,4)), dtype = 'float32')\n",
    "        pos_before = tf.convert_to_tensor(\n",
    "            np.random.random((1,4)), dtype='float32')\n",
    "\n",
    "        tmp_loss = 0\n",
    "        tmp_joint_command = joint_command\n",
    "        \n",
    "        list_pos_before.append(pos_before)\n",
    "        j = 0\n",
    "        #for j in (range(nb_pass)):\n",
    "        while (tmp_loss < 10000) and (j < 100):\n",
    "            new_joint_command = zeros((1,4))\n",
    "            #new_joint_command[0][0] = random.choice([-1,-1])*randrange(1, 0, 0.2)\n",
    "            #new_joint_command[0][1] = random.choice([-1,-1])*randrange(1, 0, 0.2)\n",
    "            #new_joint_command[0][2] = random.choice([-1,-1])*randrange(1, 0, 0.2)\n",
    "        \n",
    "            new_joint_command[0][0] = random.choice([-1,1])*0.25\n",
    "            new_joint_command[0][1] = random.choice([-1,1])*0.25\n",
    "            new_joint_command[0][2] = random.choice([-1,1])*np.sqrt(\n",
    "                0.375 -  (new_joint_command[0][0] * new_joint_command[0][0] + new_joint_command[0][1] * new_joint_command[0][1]))\n",
    "            \n",
    "            new_loss = compute_loss_vd(model, \n",
    "                                       pos_before, \n",
    "                                       new_joint_command, \n",
    "                                       filter_index)\n",
    "            \n",
    "            if (new_loss > tmp_loss) :\n",
    "                tmp_loss = new_loss\n",
    "                tmp_joint_command = new_joint_command\n",
    "            #else : \n",
    "            #    pass\n",
    "            j += 1\n",
    "        list_loss.append(tmp_loss)\n",
    "        list_joint_command.append(tmp_joint_command)\n",
    "    \n",
    "    return list_loss, list_joint_command, list_pos_before\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "model = encoder_conv\n",
    "list_loss, list_joint_command, list_pos_before = generate_max_vd(model, 35, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib notebook  \n",
    "def plot_max_vd(list_pos_before, list_joint_command):\n",
    "    fig = figure(facecolor=(0.0, 0.0, 0.0))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    for i in (range(100)):\n",
    "        if (list_loss[i] == 0) : #or (list_loss[i] < mean(list_loss)):\n",
    "            pass\n",
    "        else : \n",
    "            position_before = control_robot(list_pos_before[i][0])\n",
    "            posture_after = np.add(list_pos_before[i][0], list_joint_command[i][0])\n",
    "            position_after = control_robot(posture_after)\n",
    "            plt.quiver(position_before[0], \n",
    "                       position_before[1], \n",
    "                       position_before[2], \n",
    "                       position_after[0], \n",
    "                       position_after[1], \n",
    "                       position_after[2], \n",
    "                       length = 0.1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_zticks([])\n",
    "    plt.title('Receptive field of visuomotor neuron')\n",
    "    plt.show()\n",
    "%matplotlib notebook   \n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_max_vd(list_pos_before, list_joint_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice([-1,-1])*randrange(1, 0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_contribution(model, nb_pass):\n",
    "    pos_before1 = tf.convert_to_tensor(\n",
    "        np.random.random((1,4)), dtype='float32')\n",
    "    pos_before2 = tf.convert_to_tensor(\n",
    "        np.random.random((1,4)), dtype='float32')\n",
    "    \n",
    "    list_joint_cmd1 = []\n",
    "    list_joint_cmd2 = []\n",
    "    pos = []\n",
    "    pos.append(pos_before1)\n",
    "    pos.append(pos_before2)\n",
    "    \n",
    "    for i in tqdm.tqdm(range(32)):\n",
    "        tmp_loss1 = 0\n",
    "        tmp_loss2 = 0\n",
    "        \n",
    "        tmp_joint_command1 = zeros((1,4))\n",
    "        tmp_joint_command2 = zeros((1,4))\n",
    "        \n",
    "        for j in range(nb_pass):\n",
    "            new_joint_command1 = zeros((1,4))\n",
    "            new_joint_command1[0][0] = random.choice([-1,1])*0.25\n",
    "            new_joint_command1[0][1] = random.choice([-1,1])*0.25\n",
    "            new_joint_command1[0][2] = random.choice([-1,1])*np.sqrt(\n",
    "                0.375 -  (new_joint_command1[0][0] * new_joint_command1[0][0] + new_joint_command1[0][1] * new_joint_command1[0][1]))\n",
    "          \n",
    "        \n",
    "            new_joint_command2 = zeros((1,4))\n",
    "            new_joint_command2[0][0] = random.choice([-1,1])*0.25\n",
    "            new_joint_command2[0][1] = random.choice([-1,1])*0.25\n",
    "            new_joint_command2[0][2] = random.choice([-1,1])*np.sqrt(\n",
    "                0.375 -  (new_joint_command2[0][0] * new_joint_command2[0][0] + new_joint_command2[0][1] * new_joint_command2[0][1]))\n",
    "            \n",
    "            loss1 = compute_loss_vd(model, pos_before1, new_joint_command1, i )\n",
    "            loss2 = compute_loss_vd(model, pos_before2, new_joint_command2, i )\n",
    "            \n",
    "            if (loss1 > tmp_loss1):\n",
    "                tmp_loss1 = loss1\n",
    "                tmp_joint_command1 = new_joint_command1\n",
    "            \n",
    "            if (loss2 > tmp_loss2):\n",
    "                tmp_loss2 = loss2\n",
    "                tmp_joint_command2 = new_joint_command2\n",
    "                \n",
    "        list_joint_cmd1.append(tmp_joint_command1)\n",
    "        list_joint_cmd2.append(tmp_joint_command2)\n",
    "        \n",
    "    return pos, list_joint_cmd1, list_joint_cmd2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder = encoder_conv\n",
    "pos, list_joint_cmd1, list_joint_cmd2 = multiple_contribution(model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "def plot_multiple_contrib(pos, list_joint_cmd1, indice):\n",
    "    fig = figure(facecolor=(0.0,0.0,0.0))\n",
    "    ax = fig.gca(projection=\"3d\")\n",
    "    \n",
    "    for i in tqdm.tqdm(range(32)):\n",
    "        position_before = pos[indice][0]\n",
    "        posture_after = np.add(position_before, list_joint_cmd1[i][0])\n",
    "        position_after = control_robot(posture_after)\n",
    "        plt.quiver(position_before[0],\n",
    "                  position_before[1], \n",
    "                  position_before[2], \n",
    "                  position_after[0], \n",
    "                  position_after[1], \n",
    "                  position_after[2], \n",
    "                  length = 0.1, \n",
    "                  color = plt.cm.Set1(i/32.0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_contrib(pos, list_joint_cmd1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_contrib(pos,list_joint_cmd2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
