{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation visuelle pour reconstruction d'image corporelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "L'objectif est d'améliorer le code de network2_01.py. \n",
    "Plusieurs pistes d'amélioration sont possibles:\n",
    "1. Utiliser un produit tensoriel plutôt qu'un produit terme à terme.\n",
    "2. Différencier la cible de la main.\n",
    "3. Intégerer tf.\n",
    "4. changer la répartition des points. \n",
    "\n",
    "Je m'inspire de l'article de Memisevic, Gradient-based learning of higher-order image features et de son code gatedAutoencoder.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports et setup\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "from matplotlib.pylab import *\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from drawnow import *\n",
    "from skimage.draw import line, line_aa\n",
    "\n",
    "import time \n",
    "import random\n",
    "import scipy\n",
    "import math\n",
    "import cv2\n",
    "import cPickle as pickle\n",
    "\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size':16})\n",
    "to_backup = True\n",
    "timeframe = time.strftime('%Y%m%d%H%M%S')\n",
    "L1 = 16\n",
    "L2 = 8\n",
    "L3 = 4\n",
    "\n",
    "nb_posture = 300\n",
    "nb_command = 100\n",
    "nb_joint = 3\n",
    "nb_data = nb_command*nb_posture\n",
    "img_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction de la base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. On génère n postures différentes aléatoirement, X.\n",
    "2. On génère m commandes aléatoirement, H. \n",
    "3. On applique chaque commande à chaque posture et on obtient des nouvelles postures Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des postures initiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randrange(n , vmin, vmax):\n",
    "    return (vmax-vmin)*rand(n) + vmin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posture = zeros((nb_posture, 3))\n",
    "posture[:,0] = randrange(nb_posture, 0, pi)\n",
    "posture[:,1] = randrange(nb_posture, 0, pi)\n",
    "posture[:,2] =randrange(nb_posture, 0, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3)\n",
      "[0.96906973 0.28252661 2.43261711]\n"
     ]
    }
   ],
   "source": [
    "print(shape(posture))\n",
    "print(posture[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des commandes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = zeros((nb_command, 3))\n",
    "command[:,0] = randrange(nb_command, 0, 1)*0.5\n",
    "command[:,1] = randrange(nb_command, 0, 1)*0.5\n",
    "command[:,2] =randrange(nb_command, 0, 1)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n",
      "[0.45776798 0.17261427 0.2126592 ]\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "print(shape(command))\n",
    "print(command[0])\n",
    "print(randint(0,nb_command-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des données d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_x = zeros((nb_data, 1, 3))\n",
    "train_data_y = zeros((nb_data, 1, 3))\n",
    "train_data_h = zeros((nb_data, 1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "(30000, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_x[0][0])\n",
    "print(shape(train_data_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0 \n",
    "for i in range(nb_posture):\n",
    "    for j in range(nb_command):\n",
    "        train_data_x[idx] = posture[i]\n",
    "        idx = idx + 1\n",
    "\n",
    "idx = 0 \n",
    "for i in range(nb_posture):\n",
    "    for j in range(nb_command):\n",
    "        train_data_y[idx] = posture[i]  + command[j]\n",
    "        idx = idx + 1\n",
    "        \n",
    "idx = 0 \n",
    "for i in range(nb_posture):\n",
    "    for j in range(nb_command):\n",
    "        train_data_h[idx] = command[j]\n",
    "        idx = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_data_x 0 0 ', array([0.96906973, 0.28252661, 2.43261711]))\n",
      "('train_data_h 0 0 ', array([0.45776798, 0.17261427, 0.2126592 ]))\n",
      "('train_data_y 0 0 ', array([1.4268377 , 0.45514088, 2.64527631]))\n",
      "y = x + h\n"
     ]
    }
   ],
   "source": [
    "print('train_data_x 0 0 ', train_data_x[0][0])\n",
    "print('train_data_h 0 0 ', train_data_h[0][0])\n",
    "print('train_data_y 0 0 ', train_data_y[0][0])\n",
    "print('y = x + h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des images associées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 64\n",
    "train_images_x = zeros((nb_data, 1, img_size, img_size ), dtype = uint8)\n",
    "train_images_y = zeros((nb_data, 1, img_size, img_size ), dtype = uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb_data):\n",
    "    img = zeros((img_size,img_size), dtype = uint8)\n",
    "    angle = train_data_x[i][0] \n",
    "    row1, col1 = img_size//2 + int(floor(L1*sin(angle[0]))), img_size//2 + int(floor(L1*cos(angle[0])))\n",
    "    row2, col2 =  int(floor(L2*sin(angle[1]))) + row1, col1 + int(floor(L2*cos(angle[1])))\n",
    "    row3, col3 = int(floor(L3*sin(angle[2])))+ row2, col2 +  int(floor(L3*cos(angle[2])))\n",
    "    #r1, c1, val1 = line_aa(img_size//2,img_size//2,row1, col1)\n",
    "    #r2, c2, val2 = line_aa(row1, col1, row2, col2)\n",
    "    #r3, c3, val3 = line_aa(row2, col2, row3 , col3)\n",
    "    r1, c1 = line(img_size//2,img_size//2,row1, col1)\n",
    "    r2, c2 = line(row1, col1, row2, col2)\n",
    "    r3, c3 = line(row2, col2, row3 , col3)\n",
    "    img[r1,c1] = 1 #val1*255\n",
    "    img[r2,c2] = 1 #val2*255\n",
    "    img[r3,c3] = 1 #val3*255\n",
    "    train_images_x[i][0] = img \n",
    "\n",
    "for i in range(nb_data):\n",
    "    img = zeros((img_size,img_size), dtype = uint8)\n",
    "    angle = train_data_y[i][0] \n",
    "    row1, col1 = img_size//2 + int(floor(L1*sin(angle[0]))), img_size//2 + int(floor(L1*cos(angle[0])))\n",
    "    row2, col2 =  int(floor(L2*sin(angle[1]))) + row1, col1 + int(floor(L2*cos(angle[1])))\n",
    "    row3, col3 = int(floor(L3*sin(angle[2])))+ row2, col2 +  int(floor(L3*cos(angle[2])))\n",
    "    #r1, c1, val1 = line_aa(img_size//2,img_size//2,row1, col1)\n",
    "    #r2, c2, val2 = line_aa(row1, col1, row2, col2)\n",
    "    #r3, c3, val3 = line_aa(row2, col2, row3 , col3)\n",
    "    r1, c1 = line(img_size//2,img_size//2,row1, col1)\n",
    "    r2, c2 = line(row1, col1, row2, col2)\n",
    "    r3, c3 = line(row2, col2, row3 , col3)\n",
    "    img[r1,c1] = 1 #val1*255\n",
    "    img[r2,c2] = 1 #val2*255\n",
    "    img[r3,c3] = 1 #val3*255\n",
    "    train_images_y[i][0] = img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('taille train_images_x', (30000, 1, 64, 64))\n",
      "('taille train_images_y', (30000, 1, 64, 64))\n",
      "('taille train_features', (30000, 2, 64, 64))\n"
     ]
    }
   ],
   "source": [
    "print('taille train_images_x', shape(train_images_x))\n",
    "print('taille train_images_y', shape(train_images_y))\n",
    "train_features = concatenate((train_images_x, train_images_y), 1)\n",
    "print('taille train_features', shape(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut rajouter une gaussienne au bout de l'effecteur pour le mettre en évidence\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def makeGaussian(size, fwhm = 3, center=None):\n",
    "    \"\"\" Make a square gaussian kernel.\n",
    "    size is the length of a side of the square\n",
    "    fwhm is full-width-half-maximum, which\n",
    "    can be thought of as an effective radius.\n",
    "    \"\"\"\n",
    "    x = arange(0, size, 1, float)\n",
    "    y = x[:,newaxis]\n",
    "    if center is None:\n",
    "        x0 = y0 = size // 2\n",
    "    else:\n",
    "        x0 = center[0]\n",
    "        y0 = center[1]    \n",
    "    return exp(-4*log(2) * ((x-x0)**2 + (y-y0)**2) / fwhm**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3d47b5d4d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(train_images_x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3d47a76b10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(train_images_y[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorized Gated Field Auto-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant le passge dans l'autoencodeur, les images sont factorisées en passant par une couche de percpetron. De même, la couche latente est factorisée.\n",
    "Les images sont de taille (128,128), on prend pour commencer 32 neurones. \n",
    "La sortie est de taille (3,1) (trois moteurs), on prend pour commencer une factorisation de taille (32,1) (synérgies motrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc un encodeur, un décodeur et 3 couches de perceptrons pour les deux images et pour les commandes motrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par construire un auto encodeur dont les entrées sont un tenseur de taille (32,1) et de sortie (32,1), pour garder la symétrie de la structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Activation, Dense, Input, Multiply\n",
    "from keras.layers import Conv2D, Flatten, Reshape\n",
    "from keras.layers import Dot, Lambda, Concatenate, RepeatVector\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de l'encodeur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('taille initiale inputs_xy', (None, 2, 64, 64))\n",
      "('taille intiale de x', (None, 1, 64, 64))\n",
      "('taille initiale de y', (None, 1, 64, 64))\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "('taille inter fx', (None, 32, 1))\n",
      "('taille inter fy', (None, 32, 1))\n",
      "('taill inter matmul', (None, 32, 1))\n",
      "('out taille', (None, 1, 64))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "xy (InputLayer)                 (None, 2, 64, 64)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 64, 64)       0           xy[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 64, 64)       0           xy[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 64, 64)    0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 64, 64)    0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4096)         0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "latent_fx (Dense)               (None, 32)           131104      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "latent_fy (Dense)               (None, 32)           131104      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 32, 1)        0           latent_fx[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 32, 1)        0           latent_fy[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 32, 1)        0           reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 32)           0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "latent_fh (Dense)               (None, 32)           1056        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 32)           0           latent_fh[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 32)        0           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 32)        0           reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 64)        0           reshape_6[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 263,264\n",
      "Trainable params: 263,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_size = 128\n",
    "input_shape = (2, input_size, input_size)\n",
    "latent_dim = 32 # plus facile pour la concaténation des outputs...\n",
    "\n",
    "# on donne en entrée des pairs d'images\n",
    "inputs_xy = Input(shape = (2, img_size, img_size, ), name = 'xy')\n",
    "\n",
    "# on sépare chacune des images\n",
    "x = Lambda(lambda x: x[:,0,:,:])(inputs_xy)\n",
    "x = Reshape((1,img_size, img_size,))(x)\n",
    "\n",
    "y = Lambda(lambda x: x[:,1,:,:])(inputs_xy)\n",
    "y = Reshape((1,img_size, img_size,))(y)\n",
    "\n",
    "print('taille initiale inputs_xy', K.int_shape(inputs_xy))\n",
    "print('taille intiale de x', K.int_shape(x))\n",
    "print('taille initiale de y', K.int_shape(y))\n",
    "\n",
    "# on factorise chacune des images\n",
    "fx = Flatten()(x)\n",
    "fx = Dense(latent_dim, activation = 'relu', name = 'latent_fx')(fx)\n",
    "fx = Reshape((latent_dim,1,))(fx)\n",
    "\n",
    "fy = Flatten()(y)\n",
    "fy = Dense(latent_dim, activation = 'relu', name = 'latent_fy')(fy)\n",
    "fy = Reshape((latent_dim,1,))(fy)\n",
    "\n",
    "# on multiplie les deux factorisations, TODO mieux si produit tensoriel\n",
    "matmul = Multiply()([fx, fy])\n",
    "\n",
    "print('taille inter fx', K.int_shape(fx))\n",
    "print('taille inter fy', K.int_shape(fy))\n",
    "print('taill inter matmul', K.int_shape(matmul))\n",
    "\n",
    "# on passe le tout dans une couche de perceptrons pour obtenir les synérgies motrices\n",
    "x = Flatten()(matmul)\n",
    "fh = Dense(latent_dim, name = 'latent_fh')(x)\n",
    "fh = Reshape((latent_dim,))(fh)\n",
    "\n",
    "# on passe des synérgies motrices aux actionneurs\n",
    "#latent = Dense(3, activation = 'sigmoid',  name = 'latent_vector')(fh)\n",
    "#print('taille latent', K.int_shape(latent))\n",
    "\n",
    "\n",
    "# tricks pour pouvoir passer fx et latent en outputs\n",
    "fx = Reshape((1,latent_dim,))(fx)\n",
    "fh = Reshape((1,latent_dim,))(fh)\n",
    "#latent = Reshape((1,3,))(latent)\n",
    "\n",
    "out = Concatenate()([fx, fh])\n",
    "print('out taille', K.int_shape(out))\n",
    "\n",
    "encoder = Model(inputs = inputs_xy, outputs = out, name = 'encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du décodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('taille input', (None, 1, 64))\n",
      "('taille fx', (None, 32, 1))\n",
      "('taille tmp', (None, 32))\n",
      "('taille fhdec avant', (None, 32))\n",
      "('taille fhdec apres', (None, 32, 1))\n",
      "('taille matmuldec avant', (None, 32, 1))\n",
      "('taille matmuldec apres', (None, 32))\n",
      "('taille img dec', (None, 1, 64, 64))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, 1, 64)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 1, 32)        0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 32)           0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1, 32)        0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent_fhdec (Dense)            (None, 32)           1056        reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 32, 1)        0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 32, 1)        0           latent_fhdec[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 32, 1)        0           reshape_8[0][0]                  \n",
      "                                                                 reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 32)           0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "latent_fydec (Dense)            (None, 32)           1056        reshape_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "y_recon (Dense)                 (None, 4096)         135168      latent_fydec[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 1, 64, 64)    0           y_recon[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 137,280\n",
      "Trainable params: 137,280\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = Input(shape = (1,2*latent_dim,), name = 'decoder_input')\n",
    "print('taille input', K.int_shape(latent_inputs))\n",
    "\n",
    "# on déballe la factorisation en x\n",
    "fxdec = Lambda(lambda x: x[:,:,:latent_dim])(latent_inputs)\n",
    "fxdec = Reshape((latent_dim,1,))(fxdec)\n",
    "print('taille fx', K.int_shape(fxdec))\n",
    "\n",
    "# on déballe les commandes motrices\n",
    "inp = Lambda(lambda x: x[:,:,latent_dim:])(latent_inputs)\n",
    "inp = Reshape((latent_dim,))(inp)\n",
    "print('taille tmp', K.int_shape(inp))\n",
    "\n",
    "# on fait passer les commandes motrices dans la couche de perceptrons\n",
    "fhdec = Dense(latent_dim, name='latent_fhdec')(inp)\n",
    "print('taille fhdec avant', K.int_shape(fhdec))\n",
    "fhdec = Reshape((latent_dim,1,))(fhdec)\n",
    "print('taille fhdec apres', K.int_shape(fhdec))\n",
    "\n",
    "# on mutliplie les deux représentations\n",
    "matmuldec = Multiply()([fxdec, fhdec])\n",
    "print('taille matmuldec avant', K.int_shape(matmuldec))\n",
    "matmuldec = Reshape((latent_dim,))(matmuldec)\n",
    "print('taille matmuldec apres', K.int_shape(matmuldec))\n",
    "\n",
    "# on en déduit une factorisation \n",
    "fydec = Dense(latent_dim, name = 'latent_fydec')(matmuldec)\n",
    "\n",
    "# on déduit l'image de départ de cette factorisation\n",
    "ydec = Dense(img_size*img_size, activation = 'softmax', name = 'y_recon')(fydec)\n",
    "ydec = Reshape((1,img_size, img_size,))(ydec)\n",
    "print('taille img dec', K.int_shape(ydec))\n",
    "\n",
    "\n",
    "decoder = Model(latent_inputs, outputs= ydec, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de l'auto-encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "xy (InputLayer)              (None, 2, 64, 64)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (None, 1, 64)             263264    \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 1, 64, 64)         137280    \n",
      "=================================================================\n",
      "Total params: 400,544\n",
      "Trainable params: 400,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Model(inputs_xy, decoder(encoder(inputs_xy)), name = \"autoencoder\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enregistre des figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model \n",
    "plot_model(encoder, to_file='encoder.png')\n",
    "plot_model(decoder, to_file = 'decoder.png')\n",
    "plot_model(autoencoder, to_file = 'auto_encoder.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraîne le modèle sur des pairs d'images.\n",
    "On utilise une descente de gradient stochastique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/1000\n",
      "24000/24000 [==============================] - 5s 214us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0190 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0236\n",
      "Epoch 2/1000\n",
      "24000/24000 [==============================] - 5s 202us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0254 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0285\n",
      "Epoch 3/1000\n",
      "24000/24000 [==============================] - 5s 199us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0292 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0309\n",
      "Epoch 4/1000\n",
      "24000/24000 [==============================] - 5s 200us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0312 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0324\n",
      "Epoch 5/1000\n",
      "24000/24000 [==============================] - 5s 201us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0326 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0334\n",
      "Epoch 6/1000\n",
      "24000/24000 [==============================] - 5s 200us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0336 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0342\n",
      "Epoch 7/1000\n",
      "24000/24000 [==============================] - 5s 202us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0344 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0349\n",
      "Epoch 8/1000\n",
      "24000/24000 [==============================] - 5s 201us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0351 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0355\n",
      "Epoch 9/1000\n",
      "24000/24000 [==============================] - 5s 201us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0356 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0358\n",
      "Epoch 10/1000\n",
      "24000/24000 [==============================] - 5s 201us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0360 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0362\n",
      "Epoch 11/1000\n",
      "24000/24000 [==============================] - 5s 202us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0364 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0366\n",
      "Epoch 12/1000\n",
      "24000/24000 [==============================] - 5s 203us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0368 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0369\n",
      "Epoch 13/1000\n",
      "24000/24000 [==============================] - 5s 201us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0371 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0372\n",
      "Epoch 14/1000\n",
      "24000/24000 [==============================] - 5s 202us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0375 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0376\n",
      "Epoch 15/1000\n",
      "24000/24000 [==============================] - 5s 202us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0378 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0378\n",
      "Epoch 16/1000\n",
      "24000/24000 [==============================] - 5s 204us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0381 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0379\n",
      "Epoch 17/1000\n",
      "24000/24000 [==============================] - 5s 203us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0383 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0381\n",
      "Epoch 18/1000\n",
      "24000/24000 [==============================] - 5s 202us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0385 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0383\n",
      "Epoch 19/1000\n",
      "24000/24000 [==============================] - 5s 203us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0386 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0384\n",
      "Epoch 20/1000\n",
      "24000/24000 [==============================] - 5s 203us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0388 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0386\n",
      "Epoch 21/1000\n",
      "24000/24000 [==============================] - 5s 203us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0390 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0388\n",
      "Epoch 22/1000\n",
      "24000/24000 [==============================] - 5s 203us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0392 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0390\n",
      "Epoch 23/1000\n",
      "24000/24000 [==============================] - 5s 202us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0394 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0391\n",
      "Epoch 24/1000\n",
      "24000/24000 [==============================] - 5s 203us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0395 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0392\n",
      "Epoch 25/1000\n",
      "24000/24000 [==============================] - 5s 203us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0397 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0394\n",
      "Epoch 26/1000\n",
      "24000/24000 [==============================] - 5s 203us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0398 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0395\n",
      "Epoch 27/1000\n",
      "24000/24000 [==============================] - 5s 202us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0400 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0395\n",
      "Epoch 28/1000\n",
      "24000/24000 [==============================] - 5s 202us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0401 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0395\n",
      "Epoch 29/1000\n",
      "24000/24000 [==============================] - 5s 203us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0402 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0397\n",
      "Epoch 30/1000\n",
      "24000/24000 [==============================] - 5s 203us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0403 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0399\n",
      "Epoch 31/1000\n",
      "24000/24000 [==============================] - 5s 201us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0404 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0401\n",
      "Epoch 32/1000\n",
      "24000/24000 [==============================] - 5s 199us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0406 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0401\n",
      "Epoch 33/1000\n",
      "24000/24000 [==============================] - 5s 200us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0407 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0402\n",
      "Epoch 34/1000\n",
      "24000/24000 [==============================] - 5s 205us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0408 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0402\n",
      "Epoch 35/1000\n",
      "24000/24000 [==============================] - 5s 203us/step - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0409 - val_loss: 0.0061 - val_mean_absolute_error: 0.0064 - val_acc: 0.0403\n",
      "Epoch 36/1000\n",
      "13792/24000 [================>.............] - ETA: 1s - loss: 0.0062 - mean_absolute_error: 0.0064 - acc: 0.0411"
     ]
    }
   ],
   "source": [
    "sgd = keras.optimizers.SGD(lr = 0.1, momentum = 0.9)\n",
    "\n",
    "autoencoder.compile(loss = 'mse', \n",
    "                    optimizer = sgd, \n",
    "                    metrics = ['mae', 'acc'])\n",
    "\n",
    "history = autoencoder.fit(train_features,\n",
    "                          train_images_y,\n",
    "                          validation_split = 0.2,\n",
    "                          epochs = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Décodage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(x_decoded[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(train_images_y[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "rows, cols = 10, 30\n",
    "num = rows * cols\n",
    "imgs = np.concatenate([train_images_x[:num][0], train_images_y[:num][0], x_decoded[:num][0]])\n",
    "imgs = imgs.reshape((rows * 3, cols, img_size, img_size))\n",
    "imgs = np.vstack(np.split(imgs, rows, axis=1))\n",
    "imgs = imgs.reshape((rows * 3, -1, img_size, img_size))\n",
    "imgs = np.vstack([np.hstack(i) for i in imgs])\n",
    "imgs = (imgs * 255).astype(np.uint8)\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.title('Original images: top rows, '\n",
    "          'Corrupted Input: middle rows, '\n",
    "          'Denoised Input:  third rows')\n",
    "plt.imshow(imgs, interpolation='none', cmap='gray')\n",
    "Image.fromarray(imgs).save('corrupted_and_denoised.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
