{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation visuelle pour reconstruction d'image corporelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "L'objectif est d'améliorer le code de network2_01.py. \n",
    "Plusieurs pistes d'amélioration sont possibles:\n",
    "1. Utiliser un produit tensoriel plutôt qu'un produit terme à terme.\n",
    "2. Différencier la cible de la main.\n",
    "3. Intégerer tf.\n",
    "4. changer la répartition des points. \n",
    "\n",
    "Je m'inspire de l'article de Memisevic, Gradient-based learning of higher-order image features et de son code gatedAutoencoder.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports et setup\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "from matplotlib.pylab import *\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from drawnow import *\n",
    "from skimage.draw import line, line_aa\n",
    "\n",
    "import time \n",
    "import random\n",
    "import scipy\n",
    "import math\n",
    "import cv2\n",
    "import cPickle as pickle\n",
    "\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size':16})\n",
    "to_backup = True\n",
    "timeframe = time.strftime('%Y%m%d%H%M%S')\n",
    "L1 = 16\n",
    "L2 = 8\n",
    "L3 = 1\n",
    "\n",
    "nb_posture = 300\n",
    "nb_command = 100\n",
    "nb_joint = 3\n",
    "nb_data = nb_command*nb_posture\n",
    "img_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction de la base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. On génère n postures différentes aléatoirement, X.\n",
    "2. On génère m commandes aléatoirement, H. \n",
    "3. On applique chaque commande à chaque posture et on obtient des nouvelles postures Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des postures initiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randrange(n , vmin, vmax):\n",
    "    return (vmax-vmin)*rand(n) + vmin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posture = zeros((nb_posture, 3))\n",
    "posture[:,0] = randrange(nb_posture, 0, pi)\n",
    "posture[:,1] = randrange(nb_posture, 0, pi)\n",
    "posture[:,2] =randrange(nb_posture, 0, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3)\n",
      "[1.94868444 0.38406    1.02400179]\n"
     ]
    }
   ],
   "source": [
    "print(shape(posture))\n",
    "print(posture[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des commandes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = zeros((nb_command, 3))\n",
    "command[:,0] = randrange(nb_command, 0, 1)*0.3\n",
    "command[:,1] = randrange(nb_command, 0, 1)*0.3\n",
    "command[:,2] = 0 #randrange(nb_command, 0, 1)*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n",
      "[0.26030361 0.14827373 0.        ]\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print(shape(command))\n",
    "print(command[0])\n",
    "print(randint(0,nb_command-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des données d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_x = zeros((nb_data, 1, 3))\n",
    "train_data_y = zeros((nb_data, 1, 3))\n",
    "train_data_h = zeros((nb_data, 1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "(30000, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_x[0][0])\n",
    "print(shape(train_data_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0 \n",
    "for i in range(nb_posture):\n",
    "    for j in range(nb_command):\n",
    "        train_data_x[idx] = posture[i]\n",
    "        idx = idx + 1\n",
    "\n",
    "idx = 0 \n",
    "for i in range(nb_posture):\n",
    "    for j in range(nb_command):\n",
    "        train_data_y[idx] = posture[i]  + command[j]\n",
    "        idx = idx + 1\n",
    "        \n",
    "idx = 0 \n",
    "for i in range(nb_posture):\n",
    "    for j in range(nb_command):\n",
    "        train_data_h[idx] = command[j]\n",
    "        idx = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_data_x 0 0 ', array([1.94868444, 0.38406   , 1.02400179]))\n",
      "('train_data_h 0 0 ', array([0.26030361, 0.14827373, 0.        ]))\n",
      "('train_data_y 0 0 ', array([2.20898805, 0.53233373, 1.02400179]))\n",
      "y = x + h\n"
     ]
    }
   ],
   "source": [
    "print('train_data_x 0 0 ', train_data_x[0][0])\n",
    "print('train_data_h 0 0 ', train_data_h[0][0])\n",
    "print('train_data_y 0 0 ', train_data_y[0][0])\n",
    "print('y = x + h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des images associées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_x = zeros((nb_data, 1, img_size, img_size ), dtype = float32)\n",
    "train_images_y = zeros((nb_data, 1, img_size, img_size ), dtype = float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb_data):\n",
    "    img = zeros((img_size,img_size), dtype = uint8)\n",
    "    angle = train_data_x[i][0] \n",
    "    row1, col1 = img_size//2 + int(floor(L1*sin(angle[0]))), img_size//2 + int(floor(L1*cos(angle[0])))\n",
    "    row2, col2 =  int(floor(L2*sin(angle[1]))) + row1, col1 + int(floor(L2*cos(angle[1])))\n",
    "    row3, col3 = int(floor(L3*sin(angle[2])))+ row2, col2 +  int(floor(L3*cos(angle[2])))\n",
    "    r1, c1, val1 = line_aa(img_size//2,img_size//2,row1, col1)\n",
    "    r2, c2, val2 = line_aa(row1, col1, row2, col2)\n",
    "    r3, c3, val3 = line_aa(row2, col2, row3 , col3)\n",
    "    #r1, c1 = line(img_size//2,img_size//2,row1, col1)\n",
    "    #r2, c2 = line(row1, col1, row2, col2)\n",
    "    #r3, c3 = line(row2, col2, row3 , col3)\n",
    "    img[r1,c1] = val1*255\n",
    "    img[r2,c2] = val2*255\n",
    "    img[r3,c3] = val3*255\n",
    "    train_images_x[i][0] = img / 255.\n",
    "\n",
    "for i in range(nb_data):\n",
    "    img = zeros((img_size,img_size), dtype = uint8)\n",
    "    angle = train_data_y[i][0] \n",
    "    row1, col1 = img_size//2 + int(floor(L1*sin(angle[0]))), img_size//2 + int(floor(L1*cos(angle[0])))\n",
    "    row2, col2 =  int(floor(L2*sin(angle[1]))) + row1, col1 + int(floor(L2*cos(angle[1])))\n",
    "    row3, col3 = int(floor(L3*sin(angle[2])))+ row2, col2 +  int(floor(L3*cos(angle[2])))\n",
    "    r1, c1, val1 = line_aa(img_size//2,img_size//2,row1, col1)\n",
    "    r2, c2, val2 = line_aa(row1, col1, row2, col2)\n",
    "    r3, c3, val3 = line_aa(row2, col2, row3 , col3)\n",
    "    #r1, c1 = line(img_size//2,img_size//2,row1, col1)\n",
    "    #r2, c2 = line(row1, col1, row2, col2)\n",
    "    #r3, c3 = line(row2, col2, row3 , col3)\n",
    "    img[r1,c1] = val1*255\n",
    "    img[r2,c2] = val2*255\n",
    "    img[r3,c3] = val3*255\n",
    "    train_images_y[i][0] = img / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('taille train_images_x', (30000, 1, 64, 64))\n",
      "('taille train_images_y', (30000, 1, 64, 64))\n",
      "('taille train_features', (30000, 2, 64, 64))\n"
     ]
    }
   ],
   "source": [
    "print('taille train_images_x', shape(train_images_x))\n",
    "print('taille train_images_y', shape(train_images_y))\n",
    "train_features = concatenate((train_images_x, train_images_y), 1)\n",
    "print('taille train_features', shape(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut rajouter une gaussienne au bout de l'effecteur pour le mettre en évidence\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeGaussian(size, fwhm = 3, center=None):\n",
    "    \"\"\" Make a square gaussian kernel.\n",
    "    size is the length of a side of the square\n",
    "    fwhm is full-width-half-maximum, which\n",
    "    can be thought of as an effective radius.\n",
    "    \"\"\"\n",
    "    x = arange(0, size, 1, float)\n",
    "    y = x[:,newaxis]\n",
    "    if center is None:\n",
    "        x0 = y0 = size // 2\n",
    "    else:\n",
    "        x0 = center[0]\n",
    "        y0 = center[1]    \n",
    "    return exp(-4*log(2) * ((x-x0)**2 + (y-y0)**2) / fwhm**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2d010b3490>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(train_images_x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2c4a045a90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(train_images_y[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorized Gated Field Auto-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant le passge dans l'autoencodeur, les images sont factorisées en passant par une couche de percpetron. De même, la couche latente est factorisée.\n",
    "Les images sont de taille (128,128), on prend pour commencer 32 neurones. \n",
    "La sortie est de taille (3,1) (trois moteurs), on prend pour commencer une factorisation de taille (32,1) (synérgies motrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc un encodeur, un décodeur et 3 couches de perceptrons pour les deux images et pour les commandes motrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par construire un auto encodeur dont les entrées sont un tenseur de taille (32,1) et de sortie (32,1), pour garder la symétrie de la structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Activation, Dense, Input, Multiply\n",
    "from keras.layers import Conv2D, Flatten, Reshape\n",
    "from keras.layers import Dot, Lambda, Concatenate, RepeatVector\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de l'encodeur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "(None, 32)\n",
      "('out taille', (None, 1, 64))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "xy (InputLayer)                 (None, 2, 64, 64)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 64, 64)       0           xy[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 64, 64)       0           xy[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 64, 64)    0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 64, 64)    0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4096)         0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "latent_fx (Dense)               (None, 32)           131104      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "latent_fy (Dense)               (None, 32)           131104      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 32, 1)        0           latent_fx[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 32, 1)        0           latent_fy[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 32, 1)        0           reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 32)           0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "latent_fh (Dense)               (None, 32)           1056        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 32)           0           latent_fh[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 32)        0           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 32)        0           reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 64)        0           reshape_6[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 263,264\n",
      "Trainable params: 263,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_size = 128\n",
    "input_shape = (2, input_size, input_size)\n",
    "latent_dim = 32 # plus facile pour la concaténation des outputs...\n",
    "\n",
    "# on donne en entrée des pairs d'images\n",
    "inputs_xy = Input(shape = (2, img_size, img_size, ), name = 'xy')\n",
    "\n",
    "# on sépare chacune des images\n",
    "x = Lambda(lambda x: x[:,0,:,:])(inputs_xy)\n",
    "x = Reshape((1,img_size, img_size,))(x)\n",
    "\n",
    "y = Lambda(lambda x: x[:,1,:,:])(inputs_xy)\n",
    "y = Reshape((1,img_size, img_size,))(y)\n",
    "\n",
    "# on factorise chacune des images\n",
    "fx = Flatten()(x)\n",
    "fx = Dense(latent_dim, activation = 'relu', name = 'latent_fx')(fx)\n",
    "fx = Reshape((latent_dim,1,))(fx)\n",
    "\n",
    "fy = Flatten()(y)\n",
    "fy = Dense(latent_dim, activation = 'relu', name = 'latent_fy')(fy)\n",
    "fy = Reshape((latent_dim,1,))(fy)\n",
    "\n",
    "\n",
    "# on multiplie les deux factorisations, TODO mieux si produit tensoriel\n",
    "matmul = Multiply()([fx, fy])\n",
    "\n",
    "# on passe le tout dans une couche de perceptrons pour obtenir les synérgies motrices\n",
    "x = Flatten()(matmul)\n",
    "fh = Dense(latent_dim, name = 'latent_fh')(x)\n",
    "fh = Reshape((latent_dim,))(fh)\n",
    "print(K.int_shape(fh))\n",
    "# on passe des synérgies motrices aux actionneurs\n",
    "#latent = Dense(3, activation = 'sigmoid',  name = 'latent_vector')(fh)\n",
    "#print('taille latent', K.int_shape(latent))\n",
    "\n",
    "\n",
    "# tricks pour pouvoir passer fx et latent en outputs\n",
    "fx = Reshape((1,latent_dim,))(fx)\n",
    "fh = Reshape((1,latent_dim,))(fh)\n",
    "#latent = Reshape((1,3,))(latent)\n",
    "\n",
    "out = Concatenate()([fx, fh])\n",
    "print('out taille', K.int_shape(out))\n",
    "\n",
    "encoder = Model(inputs = inputs_xy, outputs = out, name = 'encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du décodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('taille input', (None, 1, 64))\n",
      "('taille fx', (None, 32, 1))\n",
      "('taille tmp', (None, 32))\n",
      "('taille fhdec avant', (None, 32))\n",
      "('taille fhdec apres', (None, 32, 1))\n",
      "('taille matmuldec avant', (None, 32, 1))\n",
      "('taille matmuldec apres', (None, 32))\n",
      "('taille img dec', (None, 1, 64, 64))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, 1, 64)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 1, 32)        0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 32)           0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1, 32)        0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent_fhdec (Dense)            (None, 32)           1056        reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 32, 1)        0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 32, 1)        0           latent_fhdec[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 32, 1)        0           reshape_8[0][0]                  \n",
      "                                                                 reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 32)           0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "latent_fydec (Dense)            (None, 32)           1056        reshape_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "y_recon (Dense)                 (None, 4096)         135168      latent_fydec[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 1, 64, 64)    0           y_recon[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 137,280\n",
      "Trainable params: 137,280\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = Input(shape = (1,2*latent_dim,), name = 'decoder_input')\n",
    "print('taille input', K.int_shape(latent_inputs))\n",
    "\n",
    "# on déballe la factorisation en x\n",
    "fxdec = Lambda(lambda x: x[:,:,:latent_dim])(latent_inputs)\n",
    "fxdec = Reshape((latent_dim,1,))(fxdec)\n",
    "print('taille fx', K.int_shape(fxdec))\n",
    "\n",
    "# on déballe les commandes motrices\n",
    "inp = Lambda(lambda x: x[:,:,latent_dim:])(latent_inputs)\n",
    "inp = Reshape((latent_dim,))(inp)\n",
    "print('taille tmp', K.int_shape(inp))\n",
    "\n",
    "# on fait passer les commandes motrices dans la couche de perceptrons\n",
    "fhdec = Dense(latent_dim, name='latent_fhdec')(inp)\n",
    "print('taille fhdec avant', K.int_shape(fhdec))\n",
    "fhdec = Reshape((latent_dim,1,))(fhdec)\n",
    "print('taille fhdec apres', K.int_shape(fhdec))\n",
    "\n",
    "# on mutliplie les deux représentations\n",
    "matmuldec = Multiply()([fxdec, fhdec])\n",
    "print('taille matmuldec avant', K.int_shape(matmuldec))\n",
    "matmuldec = Reshape((latent_dim,))(matmuldec)\n",
    "print('taille matmuldec apres', K.int_shape(matmuldec))\n",
    "\n",
    "# on en déduit une factorisation \n",
    "fydec = Dense(latent_dim, name = 'latent_fydec')(matmuldec)\n",
    "\n",
    "# on déduit l'image de départ de cette factorisation\n",
    "ydec = Dense(img_size*img_size, activation = 'sigmoid', name = 'y_recon')(fydec)\n",
    "ydec = Reshape((1,img_size, img_size,))(ydec)\n",
    "print('taille img dec', K.int_shape(ydec))\n",
    "\n",
    "\n",
    "decoder = Model(latent_inputs, outputs= ydec, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de l'auto-encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "xy (InputLayer)              (None, 2, 64, 64)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (None, 1, 64)             263264    \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 1, 64, 64)         137280    \n",
      "=================================================================\n",
      "Total params: 400,544\n",
      "Trainable params: 400,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Model(inputs_xy, decoder(encoder(inputs_xy)), name = \"autoencoder\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enregistre des figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model \n",
    "plot_model(encoder, to_file='encoder.png')\n",
    "plot_model(decoder, to_file = 'decoder.png')\n",
    "plot_model(autoencoder, to_file = 'auto_encoder.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-encodeur alternatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 32, 1)\n",
      "(None, 32)\n",
      "('out taille', (None, 1, 64))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "xy (InputLayer)                 (None, 2, 64, 64)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 64, 64)       0           xy[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 64, 64)       0           xy[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 1, 64, 64)    0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 1, 64, 64)    0           lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 1, 32, 32)    18464       reshape_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1, 32, 32)    18464       reshape_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 1024)         0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1024)         0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "latent_fx (Dense)               (None, 32)           32800       flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "latent_fy (Dense)               (None, 32)           32800       flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 32, 1)        0           latent_fx[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, 32, 1)        0           latent_fy[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 32, 1)        0           reshape_15[0][0]                 \n",
      "                                                                 reshape_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 32)           0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "latent_fh (Dense)               (None, 32)           1056        flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, 32)           0           latent_fh[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_18 (Reshape)            (None, 1, 32)        0           reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_19 (Reshape)            (None, 1, 32)        0           reshape_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 64)        0           reshape_18[0][0]                 \n",
      "                                                                 reshape_19[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 103,584\n",
      "Trainable params: 103,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_size = 128\n",
    "input_shape = (2, input_size, input_size)\n",
    "latent_dim = 32 # plus facile pour la concaténation des outputs...\n",
    "\n",
    "# on donne en entrée des pairs d'images\n",
    "inputs_xy = Input(shape = (2, img_size, img_size, ), name = 'xy')\n",
    "\n",
    "# on sépare chacune des images\n",
    "x = Lambda(lambda x: x[:,0,:,:])(inputs_xy)\n",
    "x = Reshape((1,img_size, img_size,))(x)\n",
    "\n",
    "y = Lambda(lambda x: x[:,1,:,:])(inputs_xy)\n",
    "y = Reshape((1,img_size, img_size,))(y)\n",
    "\n",
    "# on factorise chacune des images\n",
    "#fx = Flatten()(x)\n",
    "#fx = Dense(latent_dim, activation = 'relu', name = 'latent_fx')(fx)\n",
    "#fx = Reshape((latent_dim,1,))(fx)\n",
    "# alt1 : utiliser des conv2d\n",
    "fx = Conv2D(filters = 32, kernel_size = 3, strides = 2, activation = 'relu', padding = \"same\")(x)\n",
    "fx = Flatten()(fx)\n",
    "fx = Dense(latent_dim, activation = 'relu', name = 'latent_fx')(fx)\n",
    "fx = Reshape((latent_dim,1,))(fx)\n",
    "print(K.int_shape(fx))\n",
    "\n",
    "fy = Conv2D(filters = 32, kernel_size = 3, strides = 2, activation = 'relu', padding = \"same\")(y)\n",
    "fy = Flatten()(fy)\n",
    "fy = Dense(latent_dim, activation = 'relu', name = 'latent_fy')(fy)\n",
    "fy = Reshape((latent_dim,1,))(fy)\n",
    "\n",
    "# on multiplie les deux factorisations, TODO mieux si produit tensoriel\n",
    "matmul = Multiply()([fx, fy])\n",
    "\n",
    "# on passe le tout dans une couche de perceptrons pour obtenir les synérgies motrices\n",
    "x = Flatten()(matmul)\n",
    "fh = Dense(latent_dim, name = 'latent_fh')(x)\n",
    "fh = Reshape((latent_dim,))(fh)\n",
    "print(K.int_shape(fh))\n",
    "# on passe des synérgies motrices aux actionneurs\n",
    "#latent = Dense(3, activation = 'sigmoid',  name = 'latent_vector')(fh)\n",
    "#print('taille latent', K.int_shape(latent))\n",
    "\n",
    "\n",
    "# tricks pour pouvoir passer fx et latent en outputs\n",
    "fx = Reshape((1,latent_dim,))(fx)\n",
    "fh = Reshape((1,latent_dim,))(fh)\n",
    "#latent = Reshape((1,3,))(latent)\n",
    "\n",
    "out = Concatenate()([fx, fh])\n",
    "print('out taille', K.int_shape(out))\n",
    "\n",
    "encoder_alt = Model(inputs = inputs_xy, outputs = out, name = 'encoder')\n",
    "encoder_alt.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('taille input', (None, 1, 64))\n",
      "('taille fx', (None, 32, 1))\n",
      "('taille tmp', (None, 32))\n",
      "('taille fhdec avant', (None, 32))\n",
      "('taille fhdec apres', (None, 32, 1))\n",
      "('taille matmuldec avant', (None, 32, 1))\n",
      "('taille matmuldec apres', (None, 32))\n",
      "('taille img dec', (None, 1, 64, 64))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, 1, 64)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 1, 32)        0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_21 (Reshape)            (None, 32)           0           lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 1, 32)        0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent_fhdec (Dense)            (None, 32)           1056        reshape_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)            (None, 32, 1)        0           lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_22 (Reshape)            (None, 32, 1)        0           latent_fhdec[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 32, 1)        0           reshape_20[0][0]                 \n",
      "                                                                 reshape_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_23 (Reshape)            (None, 32)           0           multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "latent_fydec (Dense)            (None, 32)           1056        reshape_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "y_recon (Dense)                 (None, 4096)         135168      latent_fydec[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_24 (Reshape)            (None, 1, 64, 64)    0           y_recon[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 137,280\n",
      "Trainable params: 137,280\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = Input(shape = (1,2*latent_dim,), name = 'decoder_input')\n",
    "print('taille input', K.int_shape(latent_inputs))\n",
    "\n",
    "# on déballe la factorisation en x\n",
    "fxdec = Lambda(lambda x: x[:,:,:latent_dim])(latent_inputs)\n",
    "fxdec = Reshape((latent_dim,1,))(fxdec)\n",
    "print('taille fx', K.int_shape(fxdec))\n",
    "\n",
    "# on déballe les commandes motrices\n",
    "inp = Lambda(lambda x: x[:,:,latent_dim:])(latent_inputs)\n",
    "inp = Reshape((latent_dim,))(inp)\n",
    "print('taille tmp', K.int_shape(inp))\n",
    "\n",
    "# on fait passer les commandes motrices dans la couche de perceptrons\n",
    "fhdec = Dense(latent_dim, name='latent_fhdec')(inp)\n",
    "print('taille fhdec avant', K.int_shape(fhdec))\n",
    "fhdec = Reshape((latent_dim,1,))(fhdec)\n",
    "print('taille fhdec apres', K.int_shape(fhdec))\n",
    "\n",
    "# on mutliplie les deux représentations\n",
    "matmuldec = Multiply()([fxdec, fhdec])\n",
    "print('taille matmuldec avant', K.int_shape(matmuldec))\n",
    "matmuldec = Reshape((latent_dim,))(matmuldec)\n",
    "print('taille matmuldec apres', K.int_shape(matmuldec))\n",
    "\n",
    "# on en déduit une factorisation \n",
    "fydec = Dense(latent_dim, name = 'latent_fydec')(matmuldec)\n",
    "\n",
    "# on déduit l'image de départ de cette factorisation\n",
    "ydec = Dense(img_size*img_size, activation = 'softmax', name = 'y_recon')(fydec)\n",
    "ydec = Reshape((1,img_size, img_size,))(ydec)\n",
    "print('taille img dec', K.int_shape(ydec))\n",
    "\n",
    "\n",
    "decoder_alt = Model(latent_inputs, outputs= ydec, name='decoder')\n",
    "decoder_alt.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraîne le modèle sur des pairs d'images.\n",
    "On utilise une descente de gradient stochastique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 5s 204us/step - loss: 0.2482 - mean_absolute_error: 0.4974 - acc: 0.0369 - val_loss: 0.2482 - val_mean_absolute_error: 0.4973 - val_acc: 0.0366\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 4s 182us/step - loss: 0.2481 - mean_absolute_error: 0.4972 - acc: 0.0360 - val_loss: 0.2480 - val_mean_absolute_error: 0.4972 - val_acc: 0.0348\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 4s 184us/step - loss: 0.2480 - mean_absolute_error: 0.4971 - acc: 0.0345 - val_loss: 0.2479 - val_mean_absolute_error: 0.4971 - val_acc: 0.0330\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 4s 185us/step - loss: 0.2479 - mean_absolute_error: 0.4970 - acc: 0.0328 - val_loss: 0.2478 - val_mean_absolute_error: 0.4969 - val_acc: 0.0317\n",
      "Epoch 5/10\n",
      "11424/24000 [=============>................] - ETA: 2s - loss: 0.2478 - mean_absolute_error: 0.4969 - acc: 0.0316"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d1536bcf60c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                           \u001b[0mtrain_images_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                           \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                           epochs = 10)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sgd = keras.optimizers.SGD(lr = 0.1, momentum = 0.5)\n",
    "\n",
    "autoencoder.compile(loss = 'mse', \n",
    "                    optimizer = 'sgd', \n",
    "                    metrics = ['mae', 'acc'])\n",
    "\n",
    "history = autoencoder.fit(train_features,\n",
    "                          train_images_y,\n",
    "                          validation_split = 0.2,\n",
    "                          epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Décodage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_decoded = autoencoder.predict(train_features)\n",
    "imshow(x_decoded[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(train_images_y[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "rows, cols = 10, 30\n",
    "num = rows * cols\n",
    "imgs = np.concatenate([train_images_x[:num], train_images_y[:num], x_decoded[:num]])\n",
    "imgs = imgs.reshape((rows * 3, cols, img_size, img_size))\n",
    "imgs = np.vstack(np.split(imgs, rows, axis=1))\n",
    "imgs = imgs.reshape((rows * 3, -1, img_size, img_size))\n",
    "imgs = np.vstack([np.hstack(i) for i in imgs])\n",
    "imgs = (imgs * 255).astype(np.uint8)\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.title('Original images: top rows, '\n",
    "          'Corrupted Input: middle rows, '\n",
    "          'Denoised Input:  third rows')\n",
    "plt.imshow(imgs, interpolation='none', cmap='gray')\n",
    "Image.fromarray(imgs).save('corrupted_and_denoised.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape(x_decoded[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
